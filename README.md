# affective-risk-notes-human-state-inference-emotion-aware-systems-risks
This isn't a shopping feature idea.
If someone reading this text hesitates even a little, they're the right person.

It's dangerous for an AI to try to see into the human soul.
The wrong moment doesn't just make a bad suggestion — it pushes a person towards the wrong emotion.

The fundamental question we're asking in this project is:

Can a system understand the human soul, operate it without manipulating it?

If the answer isn't a clear "yes," then this system shouldn't be built.
The Problem

Today, recommendation systems do the following:

They generate more clicks

They drive more sales

This further enhances its addictive properties.

But they don't ask:

"Does this recommendation negatively affect the user's mood?"

"Is this user currently vulnerable?"

"Does a wrong combination reinforce a wrong mood?"

We find it unethical to proceed without asking these questions.
This system can make mistakes in the following areas:

It can misclassify moods.

It can mistake a temporary emotion for a permanent one.

It can lock the user into a single emotion.

It can manipulate by saying "it makes you feel good."

It can exploit psychological vulnerabilities the user isn't aware of.

If these risks are not taken seriously, we do not want to be involved in this project.
This system SHOULD NOT BE:

It should not pretend to be a psychologist.

It should not easily say "I understand you."

It should not treat emotion as a single label.

It should not claim to know more than the user.

We aim for a system that is quiet, humble, and remains silent when unsure.
Not a team.
Not a title.
Definitely not a CV.

What we're looking for:

A mind that says, "If this system gets it wrong, it bothers me."

Someone who can intuitively think about the transitions between perception, emotion, and behavior.

Someone who can think in models, whether they write code or not.

Someone who is more concerned with accuracy than speed.
While reading this text:

If you felt, “There is danger here,”
then you are the person we are looking for.
This project:

Is not ready for a pitch.

Is not ready for investors.

Is not ready for a presentation at all.

But if it works correctly, it can draw a small but significant line in the human-AI relationship.

If that line is important to you,
everything else is secondary.
Thoughtful discussion welcome via GitHub Discussions or Issues.
